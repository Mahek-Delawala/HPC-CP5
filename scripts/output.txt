Threads Per Block=32 nBlocks=1
==PROF== Connected to process 772792 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 772792
[772792] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:44:15, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           3.26
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.02
    SM Frequency                                                             cycle/usecond                         642.33
    Elapsed Cycles                                                                   cycle                          2,097
    Memory [%]                                                                           %                           1.05
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           3.26
    L1/TEX Cache Throughput                                                              %                         161.19
    L2 Cache Throughput                                                                  %                           1.05
    SM Active Cycles                                                                 cycle                           2.48
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=64 nBlocks=1
==PROF== Connected to process 772976 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 772976
[772976] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:44:20, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           2.98
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.05
    SM Frequency                                                             cycle/usecond                         662.68
    Elapsed Cycles                                                                   cycle                          1,973
    Memory [%]                                                                           %                           1.12
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           2.98
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.12
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=128 nBlocks=1
==PROF== Connected to process 773240 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 773240
[773240] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:44:25, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           2.94
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.13
    SM Frequency                                                             cycle/usecond                         714.04
    Elapsed Cycles                                                                   cycle                          2,103
    Memory [%]                                                                           %                           1.05
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           2.94
    L1/TEX Cache Throughput                                                              %                         161.19
    L2 Cache Throughput                                                                  %                           1.05
    SM Active Cycles                                                                 cycle                           2.48
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=256 nBlocks=1
==PROF== Connected to process 773336 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 773336
[773336] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:44:30, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           3.07
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.09
    SM Frequency                                                             cycle/usecond                         688.57
    Elapsed Cycles                                                                   cycle                          2,116
    Memory [%]                                                                           %                           1.06
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           3.07
    L1/TEX Cache Throughput                                                              %                         161.19
    L2 Cache Throughput                                                                  %                           1.06
    SM Active Cycles                                                                 cycle                           2.48
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=512 nBlocks=1
==PROF== Connected to process 773426 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 773426
[773426] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:44:35, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           3.10
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.12
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.00
    SM Frequency                                                             cycle/usecond                         634.34
    Elapsed Cycles                                                                   cycle                          1,970
    Memory [%]                                                                           %                           1.12
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           3.10
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.12
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=1024 nBlocks=1
==PROF== Connected to process 773752 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 773752
[773752] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:44:40, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.01
    gpu__time_duration.avg                                                         usecond                           3.33
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.04
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           2.71
    SM Frequency                                                             cycle/nsecond                           1.71
    Elapsed Cycles                                                                   cycle                          5,686
    Memory [%]                                                                           %                           0.39
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           3.33
    L1/TEX Cache Throughput                                                              %                         164.26
    L2 Cache Throughput                                                                  %                           0.39
    SM Active Cycles                                                                 cycle                           2.44
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=32 nBlocks=4
==PROF== Connected to process 774804 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 774804
[774804] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:44:46, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           3.26
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.10
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.02
    SM Frequency                                                             cycle/usecond                         649.55
    Elapsed Cycles                                                                   cycle                          2,122
    Memory [%]                                                                           %                           1.04
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           3.26
    L1/TEX Cache Throughput                                                              %                         163.64
    L2 Cache Throughput                                                                  %                           1.04
    SM Active Cycles                                                                 cycle                           2.44
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.04
    Achieved Active Warps Per SM                                                      warp                           7.70
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=64 nBlocks=4
==PROF== Connected to process 775054 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 775054
[775054] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:44:54, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.16
    gpu__time_duration.avg                                                         usecond                           3.78
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.13
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         972.88
    SM Frequency                                                             cycle/usecond                         605.97
    Elapsed Cycles                                                                   cycle                          2,290
    Memory [%]                                                                           %                           0.97
    DRAM Throughput                                                                      %                           0.16
    Duration                                                                       usecond                           3.78
    L1/TEX Cache Throughput                                                              %                         124.86
    L2 Cache Throughput                                                                  %                           0.97
    SM Active Cycles                                                                 cycle                           3.20
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.04
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=128 nBlocks=4
==PROF== Connected to process 775333 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 775333
[775333] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:45:01, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.15
    gpu__time_duration.avg                                                         usecond                           2.94
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.15
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.11
    SM Frequency                                                             cycle/usecond                         694.39
    Elapsed Cycles                                                                   cycle                          2,045
    Memory [%]                                                                           %                           1.08
    DRAM Throughput                                                                      %                           0.15
    Duration                                                                       usecond                           2.94
    L1/TEX Cache Throughput                                                              %                         124.50
    L2 Cache Throughput                                                                  %                           1.08
    SM Active Cycles                                                                 cycle                           3.21
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=256 nBlocks=4
==PROF== Connected to process 775518 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 775518
[775518] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:45:06, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.17
    gpu__time_duration.avg                                                         usecond                           2.98
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.08
    SM Frequency                                                             cycle/usecond                         687.45
    Elapsed Cycles                                                                   cycle                          2,048
    Memory [%]                                                                           %                           1.07
    DRAM Throughput                                                                      %                           0.17
    Duration                                                                       usecond                           2.98
    L1/TEX Cache Throughput                                                              %                         164.26
    L2 Cache Throughput                                                                  %                           1.07
    SM Active Cycles                                                                 cycle                           2.44
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=512 nBlocks=4
==PROF== Connected to process 775846 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 775846
[775846] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:45:12, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.18
    gpu__time_duration.avg                                                         usecond                           3.17
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.14
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.04
    SM Frequency                                                             cycle/usecond                         656.75
    Elapsed Cycles                                                                   cycle                          2,081
    Memory [%]                                                                           %                           1.06
    DRAM Throughput                                                                      %                           0.18
    Duration                                                                       usecond                           3.17
    L1/TEX Cache Throughput                                                              %                         158.24
    L2 Cache Throughput                                                                  %                           1.06
    SM Active Cycles                                                                 cycle                           2.53
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.07
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=1024 nBlocks=4
==PROF== Connected to process 775985 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 775985
[775985] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:45:18, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.16
    gpu__time_duration.avg                                                         usecond                           3.10
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.04
    SM Frequency                                                             cycle/usecond                         658.92
    Elapsed Cycles                                                                   cycle                          2,047
    Memory [%]                                                                           %                           1.13
    DRAM Throughput                                                                      %                           0.16
    Duration                                                                       usecond                           3.10
    L1/TEX Cache Throughput                                                              %                         164.26
    L2 Cache Throughput                                                                  %                           1.13
    SM Active Cycles                                                                 cycle                           2.44
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=32 nBlocks=16
==PROF== Connected to process 776607 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 776607
[776607] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:45:23, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           3.26
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.10
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.03
    SM Frequency                                                             cycle/usecond                         646.53
    Elapsed Cycles                                                                   cycle                          2,111
    Memory [%]                                                                           %                           1.05
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           3.26
    L1/TEX Cache Throughput                                                              %                         164.89
    L2 Cache Throughput                                                                  %                           1.05
    SM Active Cycles                                                                 cycle                           2.43
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=64 nBlocks=16
==PROF== Connected to process 777296 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 777296
[777296] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:45:30, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           3.17
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.01
    SM Frequency                                                             cycle/usecond                         638.26
    Elapsed Cycles                                                                   cycle                          2,023
    Memory [%]                                                                           %                           1.46
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           3.17
    L1/TEX Cache Throughput                                                              %                         124.14
    L2 Cache Throughput                                                                  %                           1.46
    SM Active Cycles                                                                 cycle                           3.22
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

Threads Per Block=128 nBlocks=16
==PROF== Connected to process 777583 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 777583
[777583] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:45:37, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.16
    gpu__time_duration.avg                                                         usecond                           3.26
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.14
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         988.24
    SM Frequency                                                             cycle/usecond                         629.77
    Elapsed Cycles                                                                   cycle                          2,056
    Memory [%]                                                                           %                           1.09
    DRAM Throughput                                                                      %                           0.16
    Duration                                                                       usecond                           3.26
    L1/TEX Cache Throughput                                                              %                         123.78
    L2 Cache Throughput                                                                  %                           1.09
    SM Active Cycles                                                                 cycle                           3.23
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.04
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=256 nBlocks=16
==PROF== Connected to process 777935 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 777935
[777935] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:45:45, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.13
    gpu__time_duration.avg                                                         usecond                           3.20
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.14
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.02
    SM Frequency                                                             cycle/usecond                         647.05
    Elapsed Cycles                                                                   cycle                          2,071
    Memory [%]                                                                           %                           1.06
    DRAM Throughput                                                                      %                           0.13
    Duration                                                                       usecond                           3.20
    L1/TEX Cache Throughput                                                              %                         124.86
    L2 Cache Throughput                                                                  %                           1.06
    SM Active Cycles                                                                 cycle                           3.20
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=512 nBlocks=16
==PROF== Connected to process 778096 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 778096
[778096] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:45:52, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.21
    gpu__time_duration.avg                                                         usecond                           3.55
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.15
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         904.50
    SM Frequency                                                             cycle/usecond                         575.57
    Elapsed Cycles                                                                   cycle                          2,045
    Memory [%]                                                                           %                           1.08
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       usecond                           3.55
    L1/TEX Cache Throughput                                                              %                         124.50
    L2 Cache Throughput                                                                  %                           1.08
    SM Active Cycles                                                                 cycle                           3.21
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=1024 nBlocks=16
==PROF== Connected to process 778443 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 778443
[778443] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:45:58, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.16
    gpu__time_duration.avg                                                         usecond                           2.94
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.14
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.15
    SM Frequency                                                             cycle/usecond                         733.40
    Elapsed Cycles                                                                   cycle                          2,160
    Memory [%]                                                                           %                           1.03
    DRAM Throughput                                                                      %                           0.16
    Duration                                                                       usecond                           2.94
    L1/TEX Cache Throughput                                                              %                         158.82
    L2 Cache Throughput                                                                  %                           1.03
    SM Active Cycles                                                                 cycle                           2.52
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=32 nBlocks=64
==PROF== Connected to process 778732 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 778732
[778732] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:46:04, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           3.81
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         873.11
    SM Frequency                                                             cycle/usecond                         548.84
    Elapsed Cycles                                                                   cycle                          2,091
    Memory [%]                                                                           %                           1.07
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           3.81
    L1/TEX Cache Throughput                                                              %                         164.89
    L2 Cache Throughput                                                                  %                           1.07
    SM Active Cycles                                                                 cycle                           2.43
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

Threads Per Block=64 nBlocks=64
==PROF== Connected to process 779502 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 779502
[779502] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:46:11, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.20
    gpu__time_duration.avg                                                         usecond                           3.26
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.14
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.00
    SM Frequency                                                             cycle/usecond                         627.80
    Elapsed Cycles                                                                   cycle                          2,050
    Memory [%]                                                                           %                           1.08
    DRAM Throughput                                                                      %                           0.20
    Duration                                                                       usecond                           3.26
    L1/TEX Cache Throughput                                                              %                         123.78
    L2 Cache Throughput                                                                  %                           1.08
    SM Active Cycles                                                                 cycle                           3.23
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=128 nBlocks=64
==PROF== Connected to process 779994 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 779994
[779994] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:46:19, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.06
    gpu__time_duration.avg                                                         usecond                           3.17
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.15
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.03
    SM Frequency                                                             cycle/usecond                         648.36
    Elapsed Cycles                                                                   cycle                          2,055
    Memory [%]                                                                           %                           1.08
    DRAM Throughput                                                                      %                           0.06
    Duration                                                                       usecond                           3.17
    L1/TEX Cache Throughput                                                              %                         122.03
    L2 Cache Throughput                                                                  %                           1.08
    SM Active Cycles                                                                 cycle                           3.28
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=256 nBlocks=64
==PROF== Connected to process 781101 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 781101
[781101] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:46:25, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.06
    gpu__time_duration.avg                                                         usecond                           3.01
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.12
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.28
    SM Frequency                                                             cycle/usecond                         808.84
    Elapsed Cycles                                                                   cycle                          2,434
    Memory [%]                                                                           %                           0.91
    DRAM Throughput                                                                      %                           0.06
    Duration                                                                       usecond                           3.01
    L1/TEX Cache Throughput                                                              %                         122.73
    L2 Cache Throughput                                                                  %                           0.91
    SM Active Cycles                                                                 cycle                           3.26
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=512 nBlocks=64
==PROF== Connected to process 781467 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 781467
[781467] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:46:30, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.08
    gpu__time_duration.avg                                                         usecond                           3.04
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.15
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.08
    SM Frequency                                                             cycle/usecond                         676.08
    Elapsed Cycles                                                                   cycle                          2,056
    Memory [%]                                                                           %                           1.07
    DRAM Throughput                                                                      %                           0.08
    Duration                                                                       usecond                           3.04
    L1/TEX Cache Throughput                                                              %                         122.73
    L2 Cache Throughput                                                                  %                           1.07
    SM Active Cycles                                                                 cycle                           3.26
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=1024 nBlocks=64
==PROF== Connected to process 781590 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 781590
[781590] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:46:34, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.06
    gpu__time_duration.avg                                                         usecond                           3.07
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.10
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.15
    SM Frequency                                                             cycle/usecond                         729.86
    Elapsed Cycles                                                                   cycle                          2,243
    Memory [%]                                                                           %                           0.98
    DRAM Throughput                                                                      %                           0.06
    Duration                                                                       usecond                           3.07
    L1/TEX Cache Throughput                                                              %                            160
    L2 Cache Throughput                                                                  %                           0.98
    SM Active Cycles                                                                 cycle                           2.50
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=32 nBlocks=256
==PROF== Connected to process 781841 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 781841
[781841] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:46:41, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.05
    gpu__time_duration.avg                                                         usecond                           2.98
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.48
    SM Frequency                                                             cycle/usecond                         940.86
    Elapsed Cycles                                                                   cycle                          2,801
    Memory [%]                                                                           %                           0.78
    DRAM Throughput                                                                      %                           0.05
    Duration                                                                       usecond                           2.98
    L1/TEX Cache Throughput                                                              %                         122.03
    L2 Cache Throughput                                                                  %                           0.78
    SM Active Cycles                                                                 cycle                           3.28
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=64 nBlocks=256
==PROF== Connected to process 782091 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 782091
[782091] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:46:46, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.06
    gpu__time_duration.avg                                                         usecond                           3.04
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.14
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.11
    SM Frequency                                                             cycle/usecond                         692.76
    Elapsed Cycles                                                                   cycle                          2,107
    Memory [%]                                                                           %                           1.05
    DRAM Throughput                                                                      %                           0.06
    Duration                                                                       usecond                           3.04
    L1/TEX Cache Throughput                                                              %                         121.69
    L2 Cache Throughput                                                                  %                           1.05
    SM Active Cycles                                                                 cycle                           3.29
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=128 nBlocks=256
==PROF== Connected to process 782445 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 782445
[782445] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:46:50, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.01
    gpu__time_duration.avg                                                         usecond                           3.04
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.09
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.34
    SM Frequency                                                             cycle/usecond                         837.45
    Elapsed Cycles                                                                   cycle                          2,546
    Memory [%]                                                                           %                           0.87
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           3.04
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           0.87
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=256 nBlocks=256
==PROF== Connected to process 782784 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 782784
[782784] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:46:55, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           2.98
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.10
    SM Frequency                                                             cycle/usecond                         695.90
    Elapsed Cycles                                                                   cycle                          2,072
    Memory [%]                                                                           %                           1.06
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           2.98
    L1/TEX Cache Throughput                                                              %                            160
    L2 Cache Throughput                                                                  %                           1.06
    SM Active Cycles                                                                 cycle                           2.50
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

Threads Per Block=512 nBlocks=256
==PROF== Connected to process 782900 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 782900
[782900] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:00, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.07
    gpu__time_duration.avg                                                         usecond                           2.94
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.15
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.10
    SM Frequency                                                             cycle/usecond                         699.05
    Elapsed Cycles                                                                   cycle                          2,059
    Memory [%]                                                                           %                           1.07
    DRAM Throughput                                                                      %                           0.07
    Duration                                                                       usecond                           2.94
    L1/TEX Cache Throughput                                                              %                         154.29
    L2 Cache Throughput                                                                  %                           1.07
    SM Active Cycles                                                                 cycle                           2.59
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=1024 nBlocks=256
==PROF== Connected to process 783257 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 783257
[783257] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:05, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.07
    gpu__time_duration.avg                                                         usecond                           3.07
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.07
    SM Frequency                                                             cycle/usecond                         668.62
    Elapsed Cycles                                                                   cycle                          2,055
    Memory [%]                                                                           %                           1.07
    DRAM Throughput                                                                      %                           0.07
    Duration                                                                       usecond                           3.07
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.07
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=32 nBlocks=1024
==PROF== Connected to process 783563 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 783563
[783563] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:09, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           2.98
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.10
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.17
    SM Frequency                                                             cycle/usecond                         736.75
    Elapsed Cycles                                                                   cycle                          2,193
    Memory [%]                                                                           %                           1.00
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           2.98
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.00
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

Threads Per Block=64 nBlocks=1024
==PROF== Connected to process 783914 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 783914
[783914] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:14, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           3.10
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.05
    SM Frequency                                                             cycle/usecond                         665.32
    Elapsed Cycles                                                                   cycle                          2,067
    Memory [%]                                                                           %                           1.07
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           3.10
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.07
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

Threads Per Block=128 nBlocks=1024
==PROF== Connected to process 784036 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 784036
[784036] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:19, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.11
    gpu__time_duration.avg                                                         usecond                           3.74
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         871.79
    SM Frequency                                                             cycle/usecond                         548.65
    Elapsed Cycles                                                                   cycle                          2,056
    Memory [%]                                                                           %                           1.10
    DRAM Throughput                                                                      %                           0.11
    Duration                                                                       usecond                           3.74
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.10
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=256 nBlocks=1024
==PROF== Connected to process 784266 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 784266
[784266] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:23, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           2.98
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.12
    SM Frequency                                                             cycle/usecond                         705.21
    Elapsed Cycles                                                                   cycle                          2,100
    Memory [%]                                                                           %                           1.05
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           2.98
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.05
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=512 nBlocks=1024
==PROF== Connected to process 784458 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 784458
[784458] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:28, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           3.04
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.11
    SM Frequency                                                             cycle/usecond                         696.38
    Elapsed Cycles                                                                   cycle                          2,118
    Memory [%]                                                                           %                           1.04
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           3.04
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.04
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

Threads Per Block=1024 nBlocks=1024
==PROF== Connected to process 784539 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 784539
[784539] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:32, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           2.98
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.13
    SM Frequency                                                             cycle/usecond                         718.94
    Elapsed Cycles                                                                   cycle                          2,140
    Memory [%]                                                                           %                           1.02
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           2.98
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.02
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

Threads Per Block=32 nBlocks=4096
==PROF== Connected to process 784762 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 784762
[784762] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:37, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.04
    gpu__time_duration.avg                                                         usecond                           3.01
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.09
    SM Frequency                                                             cycle/usecond                         681.23
    Elapsed Cycles                                                                   cycle                          2,051
    Memory [%]                                                                           %                           1.07
    DRAM Throughput                                                                      %                           0.04
    Duration                                                                       usecond                           3.01
    L1/TEX Cache Throughput                                                              %                         161.19
    L2 Cache Throughput                                                                  %                           1.07
    SM Active Cycles                                                                 cycle                           2.48
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=64 nBlocks=4096
==PROF== Connected to process 784995 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 784995
[784995] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:41, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           4.45
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.08
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         998.56
    SM Frequency                                                             cycle/usecond                         629.69
    Elapsed Cycles                                                                   cycle                          2,801
    Memory [%]                                                                           %                           1.41
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           4.45
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.41
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=128 nBlocks=4096
==PROF== Connected to process 785090 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 785090
[785090] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:46, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           2.98
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.15
    SM Frequency                                                             cycle/usecond                         718.17
    Elapsed Cycles                                                                   cycle                          2,138
    Memory [%]                                                                           %                           1.03
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           2.98
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.03
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

Threads Per Block=256 nBlocks=4096
==PROF== Connected to process 785486 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 785486
[785486] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:51, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           3.26
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         970.59
    SM Frequency                                                             cycle/usecond                         606.44
    Elapsed Cycles                                                                   cycle                          1,980
    Memory [%]                                                                           %                           1.11
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           3.26
    L1/TEX Cache Throughput                                                              %                            160
    L2 Cache Throughput                                                                  %                           1.11
    SM Active Cycles                                                                 cycle                           2.50
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=512 nBlocks=4096
==PROF== Connected to process 785863 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 785863
[785863] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:47:56, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.06
    gpu__time_duration.avg                                                         usecond                           3.01
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.11
    SM Frequency                                                             cycle/usecond                         697.57
    Elapsed Cycles                                                                   cycle                          2,099
    Memory [%]                                                                           %                           1.04
    DRAM Throughput                                                                      %                           0.06
    Duration                                                                       usecond                           3.01
    L1/TEX Cache Throughput                                                              %                         161.19
    L2 Cache Throughput                                                                  %                           1.04
    SM Active Cycles                                                                 cycle                           2.48
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.06
    Achieved Active Warps Per SM                                                      warp                           7.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

Threads Per Block=1024 nBlocks=4096
==PROF== Connected to process 785959 (/pscratch/sd/m/mahek/CP5/sobel-harness-instructional/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 785959
[785959] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-07 16:48:00, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                              0
    gpu__time_duration.avg                                                         usecond                           3.04
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.12
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.04
    SM Frequency                                                             cycle/usecond                         648.36
    Elapsed Cycles                                                                   cycle                          1,972
    Memory [%]                                                                           %                           1.12
    DRAM Throughput                                                                      %                              0
    Duration                                                                       usecond                           3.04
    L1/TEX Cache Throughput                                                              %                         160.59
    L2 Cache Throughput                                                                  %                           1.12
    SM Active Cycles                                                                 cycle                           2.49
    Compute (SM) [%]                                                                     %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.05
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              8
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

